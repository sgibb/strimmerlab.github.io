<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="content-type" content=
  "text/html; charset=iso-8859-1" />
  <meta name="KeyWords" content=
  "machine learning, statistics, Korbinian Strimmer" />
  <meta name="Author" content="Korbinian Strimmer" />
  <meta name="Description" content="Course Description" />

  <title>Grundlagen des Maschinellen Lernens und der Statistik</title>
  <link rel="stylesheet" type="text/css" href="../../../formatold.css" />
</head>

<body>
  <script type="text/javascript">
//<![CDATA[
  <!--  
   if (top.location != self.location) {top.location = self.location}
  //-->
//]]>
</script>

  <div id="page">

    <p class="c2">Grundlagen des Maschinellen Lernens und der Statistik</p>



    <p><a href="../../../korbinian.html">Korbinian Strimmer</a>, Uni Leipzig, Wintersemester 2012/13</p>


 
 <div class="note">
<u>Beginn:</u> 16. Oktober 2012 <br />
<u>Zeit:</u> Dienstag 11:00-12:30, Mittwoch 11:00-12:30<br />
<u>Ort:</u> Seminarraum 109, Rittersaal, CIP Pool H&auml;rtelstr. 16-18 <br />
<u>Modulnr.:</u> 10-202-2206 (5 LP) <br />
    </div>

 
 
   <p class="c1">Synopsis:</p>

 
<p>
Ziel der Vorlesung ist es, die konzeptionellen Grundlagen des maschinellen Lernes zu verstehen.  Ein Gro&szlig;teil der Vorlesung besch&auml;ftigt sich mit statistischen Lernverfahren und Informationstheorie.
</p>
<p>Der Kurs besteht aus einer 2-st&uuml;ndigen Vorlesung und einem Seminar.
Die Vorlesungen finden alle vor Weihnachten statt (Mittwoch und Donnerstag),
das Seminar dann anschliessend im Januar.
Die Pr&uuml;fungsleistung zur Erlangung der <strong>5 ECTS Punkte</strong> des Moduls besteht aus einem <strong>Seminarvortrag</strong> zu einer aktuellen wissenschaftlichen Arbeit (Auswahl gegen Ende November) sowie
einer <strong>m&uuml;ndlichen Pr&uuml;fung</strong> &uuml;ber den Inhalt der Vorlesung. </p>

<p>
Themen&uuml;berblick:
<ul>
<li>Zufallsvariablen und Wahrscheinlichkeitstheorie</li>
<li>Stochastische Modellierung</li>
<li>Entropy und Information</li>
<li>Maximum likelihood und Bayesianische Inferenz</li>
<li>Clustering und Klassifikation</li>
<li>Resampling Verfahren (Bootstrap und MCMC)</li>
<li>Modellwahl und Hypothesentesten</li>
<li>Hochdimensionale Statistik und Regularisierung</li>
<li>Graphische Modelle</a>
<li>Analyse r&auml;umlich-zeitlich korrellierter Daten</li>
</ul>


    <p class="c1">Empfohlene Literatur:</p>
<ul>
<li>T. Hastie, R. Tibshirani, and J. Friedman. 2009. <A HREF="http://www-stat.stanford.edu/~tibs/ElemStatLearn/index.html">The elements of statistical learning</A>. 2nd Edition. Springer.<br />
<em>Das Standardlehrbuch zu modernen statistischen Lernverfahren (PDF ist frei verf&uuml;gbar).</em>
</li>
<li>D. J. C. MacKay. 2003. <A HREF="http://www.inference.phy.cam.ac.uk/mackay/itila/">Information theory, inference, and learning algorithms</A>. Cambridge University Press.
<br />
<em>Informationstheorie und Bayesianische Inferenz (PDF ist frei verf&uuml;gbar).</em></li>
</ul>

<p>Weitere relevante Literatur und Software:</p>
<ul>

<li>A. Caticha 2008. <a href="http://arxiv.org/abs/0808.0012v1">Lectures on Probability, Entropy, and Statistical Physics</a>.<br />
<em>Vorlesungsskript u.a. zu Bayesianischer Inferenz und Entropie  (PDF frei verf&uuml;gbar).</em></li>

<li>D. M. Diez, D. D. Barr, und M. Cetinkaya-Rundel. 2012. <a href="http://www.openintro.org/stat/textbook.php">OpenIntro Statistics</a>. 2nd Edition. <br />
<em>Einf&uuml;hrendes Statistiklehrbuch (PDF frei verf&uuml;gbar).</em></li>

<li>R. O. Duda, P. E. Hart, und D. G. Stork. 2000. <a href="http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471056693.html">Pattern Classification</a>. 2nd Edition. John Wiley and Sons. <br /> 
<em>Der Klassiker zum Thema Machine Learning (erste Auflage 1973!).</em></li>

<li> E.T. Jayes. 2003. <a href="http://www.cambridge.org/de/knowledge/isbn/item1155795/">Probability Theory - The Logic of Science"</a>. Cambridge University Press. <br />
<em>Sehr bedeutendes Buch zu den Grundlagen der Bayesianischen Inferenz.</em></li>


<li>R Project for Statistical Computing: <A HREF="http://www.r-project.org">http://www.r-project.org</A> <br />
<em>Eine freie und sehr leistungsf&auml;hige Software f&uuml;r statistische Analysen.</em></li>

</ul>



<p></p>





    <p class="c1">Kursplan:</p>


<table border="1">
  <tr>
    <th>Woche</th>
    <th>Datum</th>
    <th>Besprochene Konzepte</th>
  </tr>

 <tr>
    <td> </td>
<td> </td>
    <td><strong>Teil I: Vorlesung (Seminarraum 109, nur am 24.10. und 14.11. im Rittersaal)</strong> 
    
</td>
     </tr>


    <tr>
<td>W1</td>
 <td>-</td>
 <td></td>
  </tr>

    <tr>
<td>W2</td>
 <td>Di 16. Oktober 2012</td>
 <td><strong>Was ist Statistik:</strong> Lernen aus Daten, Entwicklung der Statistik im 20 Jahrhundert.</strong> 
<strong>Was ist Wahrscheinlichkeit:</strong> Kolmogorov Axiome, Freqentistische und Bayesianische Interpretation</strong> 
<strong>Grundbegriffe:</strong> Zufallsvariable, Beobachtungen, Dichtefunktion,
                    Verteilungsfunktion,  
                    Erwartungswert, Varianz,
                    Median, Quantilfunktion,
                     Kovarianz, Korrelation, Identit&auml;ten f&uuml;r Erwartungswert und (Ko)varianz, Unabh&auml;ngigkeit, Variablentransformation, Delta Methode, Jensen
Ungleichung.</td>
  </tr>

    <tr>
<td>W3</td>
 <td>Di 23. Oktober 2012</td>
 <td><strong>Verteilungen:</strong> <a href="distributions.png">Katalog wichtiger Verteilungen</a>, Normalverteilung, Multivariate Normalverteilung, Beta Verteilung, Exponentialverteilung, Gammaverteilung, Binomialverteilung,
        Poissonverteilung, Lokationsparameter, Skalenparameter,
Varianzstabilisierung.

</td>
  </tr>


    <tr>
<td></td>
 <td>Mi 24. Oktober 2012</td>
 <td><strong>Explorative Datenanalyse:</strong> empirische CDF, Histogramm, Box-Plot, Violin Plot, Streu-Plot, qq-Plot, Ausrei&szlig;er 
<strong>Inferenz:</strong> Statistisches Lernen, Probabilistische Modellierung von Daten,
Unsicherheitverteilung Parameter,  Sch&auml;tzfunktion, Eigenschaften von Sch&auml;tzern,
 Bias, MSE, Varianz-Bias Zerlegung, Effizienz, Konsistenz, Stichprobenverteilung.
<strong>  Einfache Sch&auml;tzer:</strong>
empirischer Erwartungswert, empirische Varianz, ECDF und Histogramm als Sch&auml;tzer.
<strong>Computerdemonstration:</strong>
         <ol>
        <li><a href="r-code/varianz.R">Vergleich von Varianzsch&auml;tzern</a> </li>
          </ol>

</td>
  </tr>


    <tr>
<td>W4</td>
 <td>Di 30. Oktober 2012</td>
 <td><strong>Information:</strong> 
Kullback-Leibler Divergenz, Boltzmann Entropie, 
Shannon Entropie, Mutual Information,  Mutual Information zwischen normalverteilten Variablen, Fisher Informationsmatrix.
<strong>Hierarchie Inferenzmethoden:</strong> KL, Maximum likelihood, Kleinste Quadrate, Penalized ML,
Bayes, empirisches Bayes.</td>
  </tr>

    <tr>
<td>W5</td>
 <td>Di 6. November 2012</td>
 <td><strong>Likelihood Inferenz:</strong>  Kullback-Leibler Distanz, Approximation bei gro&szlig;en Fallzahlen,
    Maximum-Likelihood, Least-Squares,  Likelihood Funktion,
    Score Funktion, (beobachtete) Fisher Information, Mittelwert als MLE, 
    quadratische Approximation, Likelihood interval, Wald interval, Likelihood ratio,
    Transformationsinvarianz, Optimalit&auml;t f&uuml;r gro&szlig;e Stichproben, Bias.
    Cramer-Rao Ungleichung, Overfitting, Suffizienz.</td>
  </tr>

  <tr>
<td> </td>
 <td>Mi 7. November 2012</td>

 <td>
<strong>Frequentistische Fehlerabsch&auml;tzung:</strong> Delta Methode (univariat und multivariat),
    Standardkonfidenzintervalle, Bootstrapverfahren, Bootstrap-Sch&auml;tzer f&uuml;r Varianz und Bias, Bootstrap-Sch&auml;tzer
   f&uuml;r Konfidenzintervall, Bagging, Jacknife, Pr&auml;diktionsfehler, Sch&auml;tzung duch Kreuzvalidierung.

<strong>Computerdemonstration:</strong>
         <ol>
        <li><a href="r-session-1/bootstrap-examples.R">bootstrap-examples.R</a> </li>
          </ol>
</td>
  </tr> 





   <tr>
<td>W6</td>
 <td>Di 13. November 2012</td>
 <td><strong>Regularisierung und Shrinkage: </strong>Entscheidungstheorie, Risko, Verlustfunktion,
     Hochdimensionale Inferenz, "small n, large p" Daten (z.B. DNA Chips, Proteomics), 
    Stein-Paradox, 
     James-Stein Sch&auml;tzer, Dominanz, Zul&auml;ssigkeit, Shrinkage, Model Averaging, Bias-Varianz Trade-off,
     Regularisierung,  hierarchische Modelle, empirische Bayes Inferenz, Shrinkage Sch&auml;tzer f&uuml;r Varianz und
     Korrelation. Entscheidungstheorie, Bayes Risko.
<strong>Computerdemonstration:</strong> 
   <ol>
  <li><a href="r-session-2/stein.R">stein.R</a> </li> 
  <li><a href="r-session-2/shrinkage-covariance.R">shrinkage-covariance.R</a> </li> 
 <li>Beispieldaten: <a href="r-session-2/smalldata.txt">smalldata.txt</a>, 
                    <a href="r-session-2/largedata.txt">largedata.txt</a>.</li> 
</ol>
</td>
  </tr>


  

   <tr>
<td></td>
 <td>Mi 14. November 2012</td>

 <td>
<strong>Bayesianische Inferenz:</strong> 
    Bayes' Theorem, A Priori Verteilung, A Posteriori Verteilung, Kredibilit&auml;tsintervall,
    Bayesian Learning, Zusammenhang mit Shrinkage (Linearit&auml;t in Exponentialfamilie, Regularisierung),  Wahl der Priori, Kompatibilit&auml;t Priori VT und Likelihood,
Jeffreys prior, Referenz Prior, Posteriori Matching Priors. Maximum Entropie Prior.

</td>
  </tr>


<td>W7</td>
 <td>Di 20. November 2012</td>

 <td>
<strong>Sampling:</strong>
    Rejection Sampling, Importance Sampling, Markov Chain Monte Carlo (MCMC), Metropolis Algorithmus, Metropolis Hastings, Gibbs Sampling, Reversible Jump MCMC, Hamiltonian MCMC.  Approximations.

<strong>Computerdemonstration:</strong> 
         Alle Beispiele benutzen <a href="http://www.r-project.org">R</a>:
         <ol>
        <li><a href="r-session-1/monte-carlo-pi.R">monte-carlo-pi.R</a> </li>
        <li><a href="r-session-1/monte-carlo-integral.R">monte-carlo-integral.R</a> </li>
         <li><a href="r-session-1/mcmc-examples.R">mcmc-examples.R</a> </li>
         </ol>
</td>
  </tr>


<td>W8</td>
 <td>Di 27 November 2012</td>

 <td>
<strong>Statistisches Testen und Modellwahl:</strong> Nullmodell, Alternativverteilung, Mischmodell, 
Wahl des Schwellenwertes, Fisher's p-Werte (nur Nullmodell), Bayesianische Entscheidungsregel 
(Nullmodell plus Alternative), Sensitivit&auml;t, Spezifizit&auml;t, Power, Recall, 
False Discovery Rate, False Nondiscovery Rate, True Discovery Rate, Precision,
multiples Testen.</td>
  </tr>




   <tr>
<td>W9</td>
 <td>Di 4. Dezember 2012</td>
 <td>
<strong>Klassifikationsverfahren:</strong> Pr&auml;diktionsproblem, Mischmodell, Diskriminanzfunktion, Entscheidungsgrenzen, Zentroide, gemeinsame oder getrennte Kovarianzmatrizen,
     Quadratische Diskriminanzanalyse
    (QDA), Lineare Diskriminanzanalyse (LDA), Diagonale Diskriminanzanalyse (DDA), weitere Verfahren (SVM, Naive Bayes,
     logistische Regression), Variablenselection, LDA f&uuml;r zwei Klassen, t-Statistik.
      Regularisierte Diskriminanzanalyse, PAM (Tibshirani), RDA (Hastie).

<strong>Computerdemonstration:</strong> 
         Alle Beispiele benutzen <a href="http://www.r-project.org">R</a>:
         <ol>
        <li><a href="r-session-1/classification-non-nested.R">classification-non-nested.R</a> </li> 
        <li><a href="r-session-1/classification-nested-groups.R">classification-nested-groups.R</a> </li>
         </ol>

</td>
  </tr>

   <tr>
<td>W9</td>
 <td>Mi 5. Dezember 2012</td>
 <td>
<strong>Regression:</strong> Lineares Modell, Prediktoren, Response,
     Regressionskoeffizienten, Residual, RSS, Normalengleichung, 
     Least-Squares Sch&auml;tzer, ML Sch&auml;tzer, 
     Zusammenhang Regressionskoeffizient und partieller Korrelation 
     und partieller Varianz, generalisierte lineares Modell (GLM), 
     Link Funktion, Exponentialfamilie, logistische Regression und Logit Link,
     generalisiertes additives Modell (GAM),  Ridge Regression, Lasso Regression,
     L1 und L2 Penalisierung,  Dantzig Selector, lasso und LARS, Elastic Net, Variablenselektion.
</td>
  </tr>



  <tr>
<td>W10</td>
 <td>11. Dezember 2012</td>
 <td><strong>Graphische Modelle:</strong> Korrelation und partielle Korrelation,
  Gaussianische Graphische Modelle, Bayesianische Netzwerke, Kettengraphen.
  Inferenz von graphischen Modellen. Kausalit&auml;t.
</td>
  </tr>


 <tr>
<td>W11</td>
 <td>18. Dezember 2012</td>
 <td>
<strong>Zeitreihenanalyse:</strong>
    Zeitreihe, longitudinale Daten, Trend, Autocovarianz, Autokorrelation,
    Stationarit&auml;t, Variogramm, Korrelogramm, Periodogramm, Spektrum, Sch&auml;tzung
    der Autocorrelation, AR Modell, VAR Modell, State-Space Modell, ARMA, GARCH. 
<strong>R&auml;umliche Statistik:</strong>
     R&auml;umliche Daten, r&auml;umliches Modellieren, r&auml;umliche Kovarianzfunktion,
     geostatistische Modell, Stationarit&auml;t, Istropie, Gauss-Modell, r&auml;umliches GLM,
     r&auml;umliches Variogramm, Matern Kovarianzfunktion, r&auml;umliche Pr&auml;diktion,
     Kriging.<br>
 
     <strong>Computerdemonstration:</strong> 
    <ol>
    <li><a href="r-session-3/geoR-examples.R">geoR-examples.R</a> </li> 
    </ol>
</td>
  </tr>


 <tr>
    <td> </td>
<td> </td>
    <td><strong>Teil II: Seminar (CIP Pool 009)</strong> 
    
</td>
    
   </tr>




   <tr>
<td>W12</td>
    <td>Di 8. Januar 2013</td>
    <td>Seminarvortr&auml;ge </td>
    </tr>

   <tr>
<td>W12</td>
    <td>Mi 9. Januar 2013</td>
    <td></td>
    </tr>



   <tr>
<td>W13</td>
    <td>Di 15. Januar 2013</td>
    <td></td>
    </tr>

   <tr>
<td>W13</td>
    <td>Mi 16. Januar 2013</td>
    <td></td>
    </tr>


   <tr>
<td>W14</td>
    <td>Di 22. Januar 2013</td>
    <td></td>
    </tr>

   <tr>
<td>W14</td>
    <td>Mi 23. Januar 2013</td>
    <td></td>
    </tr>


 <tr>
<td>W15</td>
    <td>Di 29. Januar 2013</td>
    <td>
    <strong>M&uuml;ndliche Pr&uuml;fungen.</strong>
   </td>
    
   </tr>





</table>  

<br />

    <div class="note" style="font-size: 6pt;">  
        <em>Letzte &Auml;nderung:</em><br />
        15. Oktober 2012<br />
        <a href="http://validator.w3.org/check?uri=referer">
           <img src="../../../images/valid-xhtml11.png" alt="Valid XHTML 1.1"
           height="31" width="88" /></a>
    </div>
   <p style="clear:right;" />
  </div>
</body>
</html>
