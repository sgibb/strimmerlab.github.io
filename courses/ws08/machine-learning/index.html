<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="content-type" content=
  "text/html; charset=iso-8859-1" />
  <meta name="KeyWords" content=
  "machine learning, statistics, Korbinian Strimmer" />
  <meta name="Author" content="Korbinian Strimmer" />
  <meta name="Description" content="Course Description" />

  <title>Grundlagen des Maschinellen Lernens und der Statistik</title>
  <link rel="stylesheet" type="text/css" href="../../../styles.css" />
</head>

<body>
  <script type="text/javascript">
//<![CDATA[
  <!--  
   if (top.location != self.location) {top.location = self.location}
  //-->
//]]>
</script>

  <div id="page">

    <div id="header">Grundlagen des Maschinellen Lernens und der Statistik</div>

   <div id="main">
    <p><a href="../../../korbinian.html">Korbinian Strimmer</a>, Uni Leipzig, Wintersemester 2008/09</p>

 
 
   <p><strong>Synopsis:</strong></p>

  <span class="sidenote">Organisatorische Details zur Vorlesung (z.B. Raum und Zeit) finden Sie
    <a href="http://www.informatik.uni-leipzig.de:8080/SGML/Documents?root=ws0809&amp;page=document.vorlesungen.vorlesung|vllab~13017">im kommentierten Vorlesungsverzeichnis WS 2008/09</a>.
    </span>


<p>
Ziel der Vorlesung ist es, die konzeptionellen Grundlagen des maschinellen Lernes zu verstehen.  Ein Gro&szlig;teil der Vorlesung besch&auml;ftigt sich mit statistischen Lernverfahren und Informationstheorie.
</p>
<p>
Geplante Inhalte:
<ul>
<li>Zufallsvariablen und Wahrscheinlichkeitstheorie</li>
<li>Stochastische Modellierung</li>
<li>Entropy und Information</li>
<li>Maximum likelihood und Bayesianische Inferenz</li>
<li>Clustering und Klassifikation</li>
<li>Resampling Verfahren (Bootstrap und MCMC)</li>
<li>Modellwahl und Hypothesentesten</li>
<li>Hochdimensionale Statistik und Regularisierung</li>
<li>Analyse r&auml;umlich-zeitlich korrellierter Daten</li></ul>

    <p><strong>Empfohlene Literatur:</strong></p>

<ul>
<li>D. R. Cox. 2006. <A HREF="http://www.cambridge.org/uk/catalogue/catalogue.asp?isbn=0521685672">Principles of statistical inference</A>. CUP.</li>
<li>F. M. Dekking et al. 2005. <A HREF="http://www.springer.com/statistics/book/978-1-85233-896-1">A modern introduction to probability and statistics: understanding why and how</A>. Springer.</li>

<li>P. J. Diggle. 1990. <A HREF="http://www.oup.com/uk/catalogue/?ci=9780198522263">Time Series: A Biostatistical Introduction</A>. OUP.</li>

<li>P. J. Diggle und P. J. Ribeiro Jr. 2007. <A HREF="http://www.leg.ufpr.br/mbgbook/">Model-based geostatistics</A>. Springer.</li>


<li>T. Hastie, R. Tibshirani, and J. Friedman. 2001. <A HREF="http://www-stat.stanford.edu/~tibs/ElemStatLearn/index.html">The elements of statistical learning</A>. Springer.</li>

<li>M. L. Lavine. 2005. <A HREF="http://www.stat.duke.edu/~michael/book.html">Introduction to statistical thought</A>. </li>
<li>D. J. C. MacKay. 2003. <A HREF="http://www.inference.phy.cam.ac.uk/mackay/itila/">Information theory, inference, and learning algorithms</A>. CUP.</li>


</ul>


<p><strong>Software:</strong></p>
<ul>
<li>R Project for Statistical Computing: <A HREF="http://www.r-project.org">http://www.r-project.org</A></li></ul>

    <p><strong>Vorlesungs&uuml;bersicht:</strong></p>

<br />

<table border="1">
  <tr>
    <th>Datum</th>
    <th>Besprochene Konzepte</th>
    <th>Literatur</th>
  </tr>

  <tr>
    <td>14. Oktober</td>
    <td><strong>Grundlagen I:</strong> Zufallsvariable, Beobachtungen, Dichtefunktion,
                    Verteilungsfunktion, Normalverteilung, Lokationsparameter, Skalenparameter, 
                    Erwartungswert, Varianz,
                    Median, Quantilfunktion, Sch&auml;tzfunktion,
                    Stichprobenverteilung, Bias, MSE, Varianz-Bias Zerlegung,
                    empirischer Erwartungswert, empirische Varianz, Histogramm.
    </td>
    <td> Lavine (Kap. 1)
    </td>
   </tr>
   <tr>
    <td>21. Oktober</td>
    <td><strong>Grundlagen II:</strong> Multivariate Normalverteilung, Kovarianz, Korrelation, 
       Identit&auml;ten f&uuml;r Erwartungswert und (Ko)varianz,  Unabh&auml;ngigkeit, Shannon Entropie,
        Mutual Information,  Entropie der Normalverteilung, Mutual Information zwischen normalverteilten Variablen, Exponentialverteilung, Gammaverteilung, Binomialverteilung,
        Poissonverteilung, R Programm, Plot von Dichten und Verteilungen.</td>
    <td></td>
   </tr>
   <tr>
    <td>28. Oktober</td>
    <td><strong>Likelihood Inferenz:</strong> Statistisches Lernen,  Kullback-Leibler Distanz, Hierarchie Inferenzmethoden,
    Maximum-Likelihood, Least-Squares, Penalized Likelihood, Bayes, Likelihood Funktion,
    Score Funktion, (beobachtete) Fisher Information, Mittelwert als MLE, 
    quadratische Approximation, Likelihood interval, Wald interval, Likelihood ratio,
    Transformationsinvarianz, Optimalit&auml;t f&uuml;r gro&szlig;e Stichproben, Bias.</td>
    <td></td>
   </tr>
   <tr>
    <td>4. November</td>
    <td><strong>Bayesianische Inferenz und Sampling Strategien:</strong> 
    Bayes' Theorem, A Priori Verteilung, A Posteriori Verteilung, Kredibilit&auml;tsintervall,
    Unterschied zu klassischer Statistik (zuf&auml;llige Parameter, Wahl der Priori), Monte Carlo Algorithmen,  
    Rejection Sampling, Importance Sampling, Markov Chain Monte Carlo (MCMC), Metropolis Algorithmus, Metropolis Hastings, Gibbs Sampling, Reversible Jump MCMC, Hamiltonian MCMC.</td>
    <td>Mackay Kap. 29 (Monte Carlo Methods) und Kap. 30 (Efficient Monte Carlo Methods).</td>
   </tr>
   <tr>
    <td>11. November</td>
    <td><strong>Klassifikationsverfahren:</strong> Pr&auml;diktionsproblem, Mischmodell, Diskriminanzfunktion, Entscheidungsgrenzen, Zentroide, gemeinsame oder getrennte Kovarianzmatrizen,
     Quadratische Diskriminanzanalyse
    (QDA), Lineare Diskriminanzanalyse (LDA), Diagonale Diskriminanzanalyse (DDA), weitere Verfahren (SVM, Naive Bayes,
     logistische Regression), Variablenselection, LDA f&uuml;r zwei Klassen, t-Statistik. </td>
    <td>Hastie et al., Kap. 4.</td>
   </tr>
   <tr>
    <td>18. November</td>
    <td><strong>Frequentistische Fehlerabsch&auml;tzung:</strong> Delta Methode (univariat und multivariat),
    Standardkonfidenzintervalle, Bootstrapverfahren, Bootstrap-Sch&auml;tzer f&uuml;r Varianz und Bias, Bootstrap-Sch&auml;tzer
   f&uuml;r Konfidenzintervall, Bagging, Jacknife, Pr&auml;diktionsfehler, Sch&auml;tzung duch Kreuzvalidierung.</td>
    <td><a href="http://www.jstor.org/stable/2685844">Efron and Gong 1983 - A leisurely look at the bootstrap, the jackknife, and cross-validation. American Statistician 37:36-48.</a></td>
   </tr>
   <tr>
    <td>25. November</td>
    <td><strong>Computerdemonstration 1:</strong> 
         Alle Beispiele benutzen <a href="http://www.r-project.org">R</a>:
         <ol>
        <li><a href="r-session-1/classification-non-nested.R">classification-non-nested.R</a> </li> 
        <li><a href="r-session-1/classification-nested-groups.R">classification-nested-groups.R</a> </li>
        <li><a href="r-session-1/monte-carlo-pi.R">monte-carlo-pi.R</a> </li>
        <li><a href="r-session-1/monte-carlo-integral.R">monte-carlo-integral.R</a> </li>
        <li><a href="r-session-1/bootstrap-examples.R">bootstrap-examples.R</a> </li>
        <li><a href="r-session-1/mcmc-examples.R">mcmc-examples.R</a> </li>
         </ol>
   </td>
    <td></td>
   </tr>
   <tr>
    <td>2. Dezember</td>
    <td><strong>Shrinkage Sch&auml;tzer: </strong>Entscheidungstheorie, Risko, Verlustfunktion,
     Hochdimensionale Inferenz, "small n, large p" Daten (z.B. DNA Chips, Proteomics), 
    Stein-Paradox, 
     James-Stein Sch&auml;tzer, Dominanz, Zul&auml;ssigkeit, Shrinkage, Model Averaging, Bias-Varianz Trade-off,
     Regularisierung,  hierarchische Modelle, empirische Bayes Inferenz, Shrinkage Sch&auml;tzer f&uuml;r Varianz und
     Korrelation.

   </td>
    <td><a href="http://www-stat.stanford.edu/~ckirby/brad/other/Article1977.pdf">Efron and Morris 1977 - Stein's paradox in statistics. Scientific American 236:119-127.</a></td>
   </tr>
   <tr>
    <td>9. Dezember</td>
    <td><strong>Computerdemonstration 2:</strong> 
   <ol>
  <li><a href="r-session-2/stein.R">stein.R</a> </li> 
  <li><a href="r-session-2/shrinkage-covariance.R">shrinkage-covariance.R</a> </li> 
 <li>Beispieldaten: <a href="r-session-2/smalldata.txt">smalldata.txt</a>, 
                    <a href="r-session-2/largedata.txt">largedata.txt</a>.</li> 
</ol>
<strong>Regularisierte Klassifikation:</strong> 
    Regularisierte Diskriminanzanalyse, PAM (Tibshirani), RDA (Hastie).  <br />
<strong>Statistisches Testen:</strong> Nullmodell, Alternativverteilung, Mischmodell, 
Wahl des Schwellenwertes, Fisher's p-Werte (nur Nullmodell), Bayesianische Entscheidungsregel 
(Nullmodell plus Alternative), Sensitivit&auml;t, Spezifizit&auml;t, Power, Recall, 
False Discovery Rate, False Nondiscovery Rate, True Discovery Rate, Precision,
multiples Testen.
     </td>
    <td><a href="http://www-stat.stanford.edu/~tibs/PAM/">PAM</a>  und 
    <a href="http://biostatistics.oxfordjournals.org/cgi/content/abstract/8/1/86">RDA</a> papers,<br />
     <a href="http://www.biomedcentral.com/1471-2105/9/303/abstract">overview of FDR methods</a>.</td>
   </tr>
   <tr>
    <td>16. Dezember</td>
    <td>Dies Academicus (statt 2. Dezember)
</td>
    <td></td>
   </tr>
   <tr>
    <td>6. Januar</td>
    <td><strong>Regression:</strong> Lineares Modell, Prediktoren, Response,
     Regressionskoeffizienten, Residual, RSS, Normalengleichung, 
     Least-Squares Sch&auml;tzer, ML Sch&auml;tzer, 
     Zusammenhang Regressionskoeffizient und partieller Korrelation 
     und partieller Varianz, generalisierte lineares Modell (GLM), 
     Link Funktion, Exponentialfamilie, logistische Regression und Logit Link,
     generalisiertes additives Modell (GAM),  Ridge Regression, Lasso Regression,
     L1 und L2 Penalisierung,  Dantzig Selector, LARS, Elastic Net, Variablenselektion.
</td>
    <td>Hastie et al. (Kapitel 3)</td>
   </tr>
   <tr>
    <td>13. Januar</td>
    <td><strong>Zeitreihenanalyse:</strong>
    Zeitreihe, longitudinale Daten, Trend, Autocovarianz, Autokorrelation,
    Stationarit&auml;t, Variogramm, Korrelogramm, Periodogramm, Spektrum, Sch&auml;tzung
    der Autocorrelation, AR Modell, VAR Modell, State-Space Modell, ARMA, GARCH. 
</td>
    <td>Diggle (Kapitel 1 bis 3)</td>
   </tr>
   <tr>
    <td>20. Januar</td>
    <td>
      <strong>R&auml;umliche Statistik:</strong>
     R&auml;umliche Daten, r&auml;umliches Modellieren, r&auml;umliche Kovarianzfunktion,
     geostatistische Modell, Stationarit&auml;t, Istropie, Gauss-Modell, r&auml;umliches GLM,
     r&auml;umliches Variogramm, Matern Kovarianzfunktion, r&auml;umliche Pr&auml;diktion,
     Kriging.<br />
 
     <strong>Computerdemonstration 3:</strong> 
    <ol>
    <li><a href="r-session-3/geoR-examples.R">geoR-examples.R</a> </li> 
    </ol>
    
    </td>
    <td>Diggle und  Ribeiro Jr. (Kapitel 1-3)</td>
   </tr>
   <tr>
    <td>27. Januar</td>
    <td>R&uuml;ckblick - Ausblick</td>
    <td></td>
   </tr>



</table>  

</div>

<div id="footer">
<em>Last modified:</em>19 January, 2009<br />
        <a href="http://validator.w3.org/check?uri=referer">
           <img src="../../../images/valid-xhtml11.png" alt="Valid XHTML 1.1"
           height="31" width="88" /></a>
</div>

    
  </div>
</body>
</html>
