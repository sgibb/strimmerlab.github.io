<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="content-type" content=
  "text/html; charset=iso-8859-1" />
  <meta name="KeyWords" content=
  "machine learning, statistics, Korbinian Strimmer" />
  <meta name="Author" content="Korbinian Strimmer" />
  <meta name="Description" content="Course Description" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Grundlagen des Maschinellen Lernens und der Statistik</title>
  <link rel="stylesheet" type="text/css" href="../../../styles.css" />
</head>

<body>
  <script type="text/javascript">
//<![CDATA[
  <!--  
   if (top.location != self.location) {top.location = self.location}
  //-->
//]]>
</script>

  <div id="page">

    <div id="header">Grundlagen des Maschinellen Lernens und der Statistik</div>

<div id="main">


    <p><a href="../../../korbinian.html">Korbinian Strimmer</a>, Uni Leipzig, Wintersemester 2010/11</p>

 
 <span class="sidenote">
<u>Beginn:</u> 11. Oktober 2010 <br />
<u>Zeit:</u> Dienstag 11-12:30 <br />
<u>Ort:</u> Seminarraum 109, H&auml;rtelstr. 16-18 <br />


    </span>

 
 
   <p><strong>Synopsis:</strong></p>

 
<p>
Ziel der Vorlesung ist es, die konzeptionellen Grundlagen des maschinellen Lernes zu verstehen.  Ein Gro&szlig;teil der Vorlesung besch&auml;ftigt sich mit statistischen Lernverfahren und Informationstheorie.
</p>
<p>
Geplante Inhalte:
<ul>
<li>Zufallsvariablen und Wahrscheinlichkeitstheorie</li>
<li>Stochastische Modellierung</li>
<li>Entropy und Information</li>
<li>Maximum likelihood und Bayesianische Inferenz</li>
<li>Clustering und Klassifikation</li>
<li>Resampling Verfahren (Bootstrap und MCMC)</li>
<li>Modellwahl und Hypothesentesten</li>
<li>Hochdimensionale Statistik und Regularisierung</li>
<li>Analyse r&auml;umlich-zeitlich korrellierter Daten</li></ul>


    <p><strong>Empfohlene Literatur:</strong></p>

<ul>
<li>D. R. Cox. 2006. <A HREF="http://www.cambridge.org/uk/catalogue/catalogue.asp?isbn=0521685672">Principles of statistical inference</A>. CUP.</li>
<li>F. M. Dekking et al. 2005. <A HREF="http://www.springer.com/statistics/book/978-1-85233-896-1">A modern introduction to probability and statistics: understanding why and how</A>. Springer.</li>

<li>P. J. Diggle. 1990. <A HREF="http://www.oup.com/uk/catalogue/?ci=9780198522263">Time Series: A Biostatistical Introduction</A>. OUP.</li>

<li>P. J. Diggle und P. J. Ribeiro Jr. 2007. <A HREF="http://www.leg.ufpr.br/mbgbook/">Model-based geostatistics</A>. Springer.</li>


<li>T. Hastie, R. Tibshirani, and J. Friedman. 2001. <A HREF="http://www-stat.stanford.edu/~tibs/ElemStatLearn/index.html">The elements of statistical learning</A>. Springer.</li>

<li>M. L. Lavine. 2005. <A HREF="http://www.stat.duke.edu/~michael/book.html">Introduction to statistical thought</A>. </li>
<li>D. J. C. MacKay. 2003. <A HREF="http://www.inference.phy.cam.ac.uk/mackay/itila/">Information theory, inference, and learning algorithms</A>. CUP.</li>

<li>R Project for Statistical Computing: <A HREF="http://www.r-project.org">http://www.r-project.org</A></li>

</ul>



<p>Parallel zur Vorlesung entsteht ein Skript (in Englisch) mit dem Titel
<a href="../../../statisticalthinking/index.html">"Statistical Thinking"</a>, in dem Sie alle wichtigen Formel
und Konzepte nachlesen k&ouml;nnen. Das f&uuml;r jede Vorlesung relevante Material
finden in den untenstehenden Links.</p>   






    <p><strong>Vorlesungs&uuml;bersicht:</strong></p>

<br />

<table border="1">
  <tr>
    <th>Woche</th>
    <th>Datum</th>
    <th>Besprochene Konzepte</th>
    <th>Literatur</th>
  </tr>

   

   <tr>
<td>W1</td>
 <td>12. Oktober 2010</td>
 <td><strong>Was ist Statistik:</strong> Lernen aus Daten, Entwicklung der Statistik im 20 Jahrhundert.

<strong>Zufallsvariablen:</strong> Zufallsvariable, Beobachtungen, Dichtefunktion,
                    Verteilungsfunktion,  
                    Erwartungswert, Varianz,
                    Median, Quantilfunktion,
                     Kovarianz, Korrelation.</td>
 <td><a href="../../../statisticalthinking/pdf/c2.pdf">Kap. 2 in "Statistical Thinking"</a></td>
  </tr>


   <tr>
<td>W2</td>
 <td>19. Oktober 2010</td>

 <td>
<strong>Zufallsvariablen II:</strong>
Identit&auml;ten f&uuml;r Erwartungswert und (Ko)varianz, Unabh&auml;ngigkeit, Variablentransformation, Delta Methode, Jensen
Ungleichung.
<strong>Verteilungen:</strong> Normalverteilung, Multivariate Normalverteilung, Exponentialverteilung, Gammaverteilung, Binomialverteilung,
        Poissonverteilung, Lokationsparameter, Skalenparameter,
Varianzstabilisierung.</td>
 <td><a href="distributions.png">Katalog wichtiger Verteilungen</a></td>
  </tr>

   <tr>
<td>W3</td>
 <td>26. Oktober 2010</td>
 <td><strong>Information:</strong> 
Kullback-Leibler Divergenz, Boltzmann Entropie, 
Shannon Entropie, Mutual Information,  Mutual Information zwischen normalverteilten Variablen, Fisher Informationsmatrix</td>
 <td><a href="../../../statisticalthinking/pdf/c4.pdf">Kap. 4 in "Statistical Thinking"</a></td>
  </tr>



  <tr>
<td>W4</td>
 <td>2. November 2010</td>
 <td>
<strong>Explorative Datenanalyse:</strong> empirische CDF, Histogramm, Box-Plot, Violin Plot Scatter-Plot, qq-Plot, Ausrei&szlig;er 
<strong>Inferenz:</strong> Statistisches Lernen, Probabilistische Modellierung von Daten,
Unsicherheitverteilung Parameter,  Sch&auml;tzfunktion, Eigenschaften von Sch&auml;tzern,
 Bias, MSE, Varianz-Bias Zerlegung, Effizienz, Konsistenz, Stichprobenverteilung,
Hierarchie Inferenzmethoden: KL, Maximum likelihood, Kleinste Quadrate, Penalized ML,
Bayes, empirisches Bayes.  Einfache Sch&auml;tzer:
empirischer Erwartungswert, empirische Varianz, ECDF und Histogramm als Sch&auml;tzer.</td>
 <td></td>
  </tr>


   <tr>
<td>W5</td>
    <td>9. November 2010</td>
    <td><strong>Likelihood Inferenz:</strong>  Kullback-Leibler Distanz, Approximation grosse Stichprobeb,
    Maximum-Likelihood, Least-Squares,  Likelihood Funktion,
    Score Funktion, (beobachtete) Fisher Information, Mittelwert als MLE, 
    quadratische Approximation, Likelihood interval, Wald interval, Likelihood ratio,
    Transformationsinvarianz, Optimalit&auml;t f&uuml;r gro&szlig;e Stichproben, Bias.
    Cramer-Rao Ungleichung, Overfitting, Suffizienz.</td>
    <td><a href="../../../statisticalthinking/pdf/c7.pdf">Kap. 7 in "Statistical Thinking"</a></td>
   </tr>


 <tr>
<td>W6</td>
    <td>16. November 2010</td>
    <td><strong>Regularisierung und Shrinkage: </strong>Entscheidungstheorie, Risko, Verlustfunktion,
     Hochdimensionale Inferenz, "small n, large p" Daten (z.B. DNA Chips, Proteomics), 
    Stein-Paradox, 
     James-Stein Sch&auml;tzer, Dominanz, Zul&auml;ssigkeit, Shrinkage, Model Averaging, Bias-Varianz Trade-off,
     Regularisierung,  hierarchische Modelle, empirische Bayes Inferenz, Shrinkage Sch&auml;tzer f&uuml;r Varianz und
     Korrelation. Entscheidungstheorie, Bayes Risko.
<strong>Computerdemonstration:</strong> 
   <ol>
  <li><a href="r-session-2/stein.R">stein.R</a> </li> 
  <li><a href="r-session-2/shrinkage-covariance.R">shrinkage-covariance.R</a> </li> 
 <li>Beispieldaten: <a href="r-session-2/smalldata.txt">smalldata.txt</a>, 
                    <a href="r-session-2/largedata.txt">largedata.txt</a>.</li> 
</ol>

   </td>
    <td><a href="http://www-stat.stanford.edu/~ckirby/brad/other/Article1977.pdf">Efron and Morris 1977 - Stein's paradox in statistics. Scientific American 236:119-127.</a></td>
   </tr>


<tr>
<td>W7</td>
    <td>23. November 2010</td>
    <td>enf&auml;llt
</td>
    <td></td>
   </tr>




<tr>
<td>W8</td>
    <td>30. November 2010</td>
    <td><strong>Frequentistische Fehlerabsch&auml;tzung:</strong> Delta Methode (univariat und multivariat),
    Standardkonfidenzintervalle, Bootstrapverfahren, Bootstrap-Sch&auml;tzer f&uuml;r Varianz und Bias, Bootstrap-Sch&auml;tzer
   f&uuml;r Konfidenzintervall, Bagging, Jacknife, Pr&auml;diktionsfehler, Sch&auml;tzung duch Kreuzvalidierung.

<strong>Computerdemonstration:</strong> 
         Alle Beispiele benutzen <a href="http://www.r-project.org">R</a>:
         <ol>
        <li><a href="r-session-1/bootstrap-examples.R">bootstrap-examples.R</a> </li>
          </ol>
</td>
    <td><a href="http://www.jstor.org/stable/2685844">Efron and Gong 1983 - A leisurely look at the bootstrap, the jackknife, and cross-validation. American Statistician 37:36-48.</a></td>
   </tr>


   <tr>
<td>W9</td>
    <td>7. Dezember 2010</td>
    <td><strong>Bayesianische Inferenz und Sampling Strategien:</strong> 
    Bayes' Theorem, A Priori Verteilung, A Posteriori Verteilung, Kredibilit&auml;tsintervall,
    Unterschied zu klassischer Statistik (zuf&auml;llige Parameter), Bayesian
Learning, Zusammenhang mit Shrinkage (Linearit&auml;t in Exponentialfamilie, Regularisierung),  Wahl der Priori, Kompatibilit&auml;t Priori VT und Likelihood,
Jeffreys prior, Referenz Prior, Posteriori Matching Priors.
    Rejection Sampling, Importance Sampling, Markov Chain Monte Carlo (MCMC), Metropolis Algorithmus, Metropolis Hastings, Gibbs Sampling, Reversible Jump MCMC, Hamiltonian MCMC.

<strong>Computerdemonstration:</strong> 
         Alle Beispiele benutzen <a href="http://www.r-project.org">R</a>:
         <ol>
        <li><a href="r-session-1/monte-carlo-pi.R">monte-carlo-pi.R</a> </li>
        <li><a href="r-session-1/monte-carlo-integral.R">monte-carlo-integral.R</a> </li>
         <li><a href="r-session-1/mcmc-examples.R">mcmc-examples.R</a> </li>
         </ol>

</td>
    <td>Mackay Kap. 29 (Monte Carlo Methods) und Kap. 30 (Efficient Monte Carlo Methods).</td>
   </tr>




 

   <tr>
<td>W10</td>
    <td>14. Dezember 2010</td>
    <td>
<strong>Statistisches Testen und Modellwahl:</strong> Nullmodell, Alternativverteilung, Mischmodell, 
Wahl des Schwellenwertes, Fisher's p-Werte (nur Nullmodell), Bayesianische Entscheidungsregel 
(Nullmodell plus Alternative), Sensitivit&auml;t, Spezifizit&auml;t, Power, Recall, 
False Discovery Rate, False Nondiscovery Rate, True Discovery Rate, Precision,
multiples Testen.
     </td>
    <td><a href="http://www-stat.stanford.edu/~tibs/PAM/">PAM</a>  und 
    <a href="http://biostatistics.oxfordjournals.org/cgi/content/abstract/8/1/86">RDA</a> papers,<br />
     <a href="http://www.biomedcentral.com/1471-2105/9/303/abstract">overview of FDR methods</a>.</td>
   </tr>


 
  <tr>
    <td> </td>
<td> </td>
    <td><strong>Frohe Weihnachten und eine guten Rutsch ins Jahr 2011!</strong> 
    
</td>
    <td></td>
   </tr>


   <tr>
<td>W11</td>
    <td>4. Januar 2011</td>
    <td><strong>Klassifikationsverfahren:</strong> Pr&auml;diktionsproblem, Mischmodell, Diskriminanzfunktion, Entscheidungsgrenzen, Zentroide, gemeinsame oder getrennte Kovarianzmatrizen,
     Quadratische Diskriminanzanalyse
    (QDA), Lineare Diskriminanzanalyse (LDA), Diagonale Diskriminanzanalyse (DDA), weitere Verfahren (SVM, Naive Bayes,
     logistische Regression), Variablenselection, LDA f&uuml;r zwei Klassen, t-Statistik.
      Regularisierte Diskriminanzanalyse, PAM (Tibshirani), RDA (Hastie).

<strong>Computerdemonstration:</strong> 
         Alle Beispiele benutzen <a href="http://www.r-project.org">R</a>:
         <ol>
        <li><a href="r-session-1/classification-non-nested.R">classification-non-nested.R</a> </li> 
        <li><a href="r-session-1/classification-nested-groups.R">classification-nested-groups.R</a> </li>
         </ol>

 </td>
    <td>Hastie et al., Kap. 4.</td>
   </tr>
   
  
   <tr>
<td>W12</td>
    <td>11. Januar 2011</td>
    <td><strong>Regression:</strong> Lineares Modell, Prediktoren, Response,
     Regressionskoeffizienten, Residual, RSS, Normalengleichung, 
     Least-Squares Sch&auml;tzer, ML Sch&auml;tzer, 
     Zusammenhang Regressionskoeffizient und partieller Korrelation 
     und partieller Varianz, generalisierte lineares Modell (GLM), 
     Link Funktion, Exponentialfamilie, logistische Regression und Logit Link,
     generalisiertes additives Modell (GAM),  Ridge Regression, Lasso Regression,
     L1 und L2 Penalisierung,  Dantzig Selector, LARS, Elastic Net, Variablenselektion.
</td>
    <td>Hastie et al. (Kapitel 3)</td>
   </tr>
   <tr>
<td>W13</td>
    <td>18. Januar 2011</td>
    <td><strong>Zeitreihenanalyse:</strong>
    Zeitreihe, longitudinale Daten, Trend, Autocovarianz, Autokorrelation,
    Stationarit&auml;t, Variogramm, Korrelogramm, Periodogramm, Spektrum, Sch&auml;tzung
    der Autocorrelation, AR Modell, VAR Modell, State-Space Modell, ARMA, GARCH. 
</td>
    <td>Diggle (Kapitel 1 bis 3)</td>
   </tr>
   <tr>
    <td>W14</td>
    <td>25. Januar 2011</td>
    <td>
      <strong>R&auml;umliche Statistik:</strong>
     R&auml;umliche Daten, r&auml;umliches Modellieren, r&auml;umliche Kovarianzfunktion,
     geostatistische Modell, Stationarit&auml;t, Istropie, Gauss-Modell, r&auml;umliches GLM,
     r&auml;umliches Variogramm, Matern Kovarianzfunktion, r&auml;umliche Pr&auml;diktion,
     Kriging.<br />
 
     <strong>Computerdemonstration:</strong> 
    <ol>
    <li><a href="r-session-3/geoR-examples.R">geoR-examples.R</a> </li> 
    </ol>
    
    </td>
    <td>Diggle und  Ribeiro Jr. (Kapitel 1-3)</td>
   </tr>
 

  <tr>
    <td>W15</td>
    <td>1. Februar 2011</td>
    <td><strong>Pr&uuml;fungen</strong>  
    </td>
    <td>
    </td>
   </tr>




</table>  

</div>

<div id="footer">
<em>Last modified:</em>20 November, 2010<br />
        <a href="http://validator.w3.org/check?uri=referer">
           <img src="../../../images/valid-xhtml11.png" alt="Valid XHTML 1.1"
           height="31" width="88" /></a>
</div>
    
  </div>
</body>
</html>
